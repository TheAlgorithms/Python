# Нейронные сети

Нейронная сеть — это вычислительная модель, вдохновлённая структурой и функциями биологических нейронных сетей. Она состоит из взаимосвязанных узлов (нейронов), которые обрабатывают информацию и передают сигналы друг другу. Нейронные сети являются основой современного глубокого обучения.

***Примечание: Некоторые файлы в этом каталоге были помечены как `.broken.txt` или `.py_tf` и были пропущены или требуют особого внимания.***

## Реализации

### Основные архитектуры
*   **Простая нейронная сеть (`simple_neural_network.py`)**: Минималистичный пример, демонстрирующий концепцию нейронной сети с одним нейроном, который обучается подстраивать один весовой коэффициент.
*   **Нейронная сеть с обратным распространением ошибки (`back_propagation_neural_network.py`)**: Реализация с нуля фреймворка для построения и обучения многослойной нейронной сети с использованием алгоритма обратного распространения ошибки.
*   **Нейронная сеть с двумя скрытыми слоями (`two_hidden_layers_neural_network.py`)**: Ещё одна реализация с нуля нейронной сети с двумя скрытыми слоями, демонстрирующая прямой проход (feedforward) и обратное распространение ошибки (backpropagation).
*   **Свёрточная нейронная сеть (CNN) (`convolution_neural_network.py`)**: Реализация с нуля гибридной CNN, которая включает свёрточный и пулинговый слои для извлечения признаков, за которыми следует полносвязная сеть для классификации.
*   **Генеративно-состязательная сеть (GAN) (`gan.py_tf`)**: Реализация с нуля генеративно-состязательной сети на `numpy` для генерации изображений рукописных цифр (MNIST). Включает в себя сети Генератора и Дискриминатора, а также оптимизатор Adam.
*   **Сеть с долгой краткосрочной памятью (LSTM) (`lstm/`)**: Пример использования библиотеки Keras для построения LSTM-сети для прогнозирования временных рядов.
*   **Многослойный перцептрон (`multilayer_perceptron_classifier.py`)**: Пример использования `MLPClassifier` из библиотеки `scikit-learn` для решения задачи классификации.

### Функции активации (`activation_functions/`)
Это набор реализаций различных функций активации, которые определяют выходной сигнал нейрона.
*   **Binary Step**: Простейшая пороговая функция.
*   **ReLU (Rectified Linear Unit)**: Самая популярная функция активации, `f(x) = max(0, x)`.
*   **Leaky ReLU**: Вариация ReLU, которая позволяет небольшой градиент для отрицательных входов.
*   **ELU (Exponential Linear Unit)**: Альтернатива ReLU, которая может принимать отрицательные значения.
*   **SELU (Scaled Exponential Linear Unit)**: Самонормализующаяся версия ELU.
*   **Softplus**: Гладкая аппроксимация ReLU.
*   **Squareplus**: Ещё одна гладкая аппроксимация ReLU.
*   **GELU (Gaussian Error Linear Unit)**: Высокопроизводительная функция, взвешивающая входы по их значению.
*   **Swish (SiLU)**: Саморегулируемая функция, `f(x) = x * sigmoid(x)`.
*   **Mish**: Гладкая, немонотонная функция активации.
*   **Soboleva Modified Hyperbolic Tangent**: Гибкая, параметризованная функция активации.

### Вспомогательные концепции
*   **Автоматическое дифференцирование (`input_data.py`)**: Скрипт, который, по-видимому, используется для загрузки данных (например, MNIST) для других моделей.
*   **Входные данные (`input_data.py`)**: Скрипт, который, по-видимому, используется для загрузки данных (например, MNIST) для других моделей.
